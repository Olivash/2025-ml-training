defaults:
- data: zarr
- dataloader: native_grid
- datamodule: ens
- diagnostics: evaluation
- hardware: example
- graph: multi_scale
- model: graphtransformer_ens
- training: ensemble
- _self_

config_validation: True

# Hardware configuration for ensemble training
hardware:
  paths:
    data: /datasets/
    output: /home/student/aifs/
  files:
    dataset: aifs-ea-an-oper-0001-mars-o48-2010-2022-6h-v1-toy.zarr
  accelerator: auto
  num_gpus_per_ensemble: 1
  num_gpus_per_model: 1

# Data configuration
data:
  resolution: o48
  timestep: 6h

# Dataloader configuration
dataloader:
  dataset: 
    dataset: ${hardware.files.dataset}
    select:  ['z_1000', 'z_500', 'z_700', 'z_300', '2t', 't_850', 'tcw', 'z_250', 'lsm', 'z', 'cp', 'tp', "cos_latitude", "cos_longitude", "sin_latitude", "sin_longitude", "cos_julian_day", "cos_local_time", "sin_julian_day", "sin_local_time", "insolation", "lsm", "sdor", "slor", "z"]

  batch_size:
    training: 2
    validation: 2
  limit_batches:
    training: 8
    validation: 8

# Datamodule configuration for ensemble training
datamodule:
  _target_: anemoi.training.data.datamodule.AnemoiEnsDatasetsDataModule

# Model configuration for ensemble training
model:
  num_channels: 128  # Reduced for debug run

  model:
    _target_: anemoi.models.models.AnemoiEnsModelEncProcDec

  # Noise injector
  noise_injector:
    _target_: anemoi.models.layers.ensemble.NoiseConditioning
    noise_std: 1
    noise_channels_dim: 4
    noise_mlp_hidden_dim: 32
    inject_noise: True
    layer_kernels:
      Activation:
        _target_: torch.nn.GELU

  # Processor with conditional layer norm
  processor:
    _target_: anemoi.models.layers.processor.GraphTransformerProcessor
    trainable_size: ${model.trainable_parameters.hidden2hidden}
    sub_graph_edge_attributes: ${model.attributes.edges}
    num_layers: 16
    num_chunks: 2
    mlp_hidden_ratio: 4 # GraphTransformer or Transformer only
    num_heads: 16 # GraphTransformer or Transformer only
    qk_norm: True # Transformer and GraphTransformer only
    cpu_offload: ${model.cpu_offload}
    layer_kernels:
      LayerNorm:
        _target_: anemoi.models.layers.normalization.ConditionalLayerNorm
        normalized_shape: ${model.num_channels}
        condition_shape: ${model.noise_injector.noise_channels_dim}
        w_one_bias_zero_init: True
        autocast: false
      Linear:
        _target_: torch.nn.Linear
      Activation:
        _target_: torch.nn.GELU
      QueryNorm:
        _target_: anemoi.models.layers.normalization.AutocastLayerNorm
        bias: False
      KeyNorm:
        _target_: anemoi.models.layers.normalization.AutocastLayerNorm
        bias: False


# Training configuration for ensemble CRPS training
training:
  # select model task
  model_task: anemoi.training.train.tasks.GraphEnsForecaster
  ensemble_size_per_device: 4

  # Fair CRPS loss function for the model
  training_loss:
    _target_: anemoi.training.losses.kcrps.AlmostFairKernelCRPS
    scalers: ['pressure_level', 'general_variable', 'nan_mask_weights', 'node_weights']
    ignore_nans: False
    alpha: 1.0

  # Fair CRPS validation metrics
  validation_metrics:
    fkcrps:
      _target_: anemoi.training.losses.kcrps.AlmostFairKernelCRPS
      scalers: ['node_weights']
      ignore_nans: False
      alpha: 1.0

  # select strategy
  strategy:
    _target_: anemoi.training.distributed.strategy.DDPEnsGroupStrategy
    num_gpus_per_ensemble: ${hardware.num_gpus_per_ensemble}
    num_gpus_per_model: ${hardware.num_gpus_per_model}
    read_group_size: ${dataloader.read_group_size}

  # Training parameters
  max_steps: 16  # Short run for demo
  max_epochs: null
  lr:
    warmup: 1000
    rate: 0.625e-4  # effective_lr=1e-3, effective_bs=16
    iterations: ${training.max_steps}
    min: 3e-7


# Diagnostics configuration
diagnostics:
  profiler: False
  enable_progress_bar: True
  callbacks: []
  plot:
    callbacks: []
  log:
    mlflow:
      system: True
