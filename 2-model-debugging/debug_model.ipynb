{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b28f9a2",
   "metadata": {},
   "source": [
    "# Debugging Machine Learning Models in PyTorch\n",
    "Welcome to the hands-on session for ML model debugging! In this notebook, you'll learn practical techniques to diagnose and fix issues in neural networks using PyTorch.\n",
    "\n",
    "**Session Outline (75 min):**\n",
    "1. Introduction to debugging ML models (5 min)\n",
    "2. Setup & imports (5 min)\n",
    "3. Visualizing data and model (5 min)\n",
    "4. Forward/backward pass debugging (10 min)\n",
    "5. Model Evaluation (10 min)\n",
    "5. Common pitfalls and how to fix them (15 min)\n",
    "6. Practical debugging tools (10 min)\n",
    "7. Guided exercise: fix a buggy model (10 min)\n",
    "8. Wrap-up & Q&A (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29028f5",
   "metadata": {},
   "source": [
    "## 1. Introduction to Debugging ML Models\n",
    "Debugging is a critical skill for any machine learning practitioner. Even well-designed models can fail due to subtle bugs, data issues, or training instabilities.\n",
    "\n",
    "In this session, you will learn:\n",
    "- How to systematically diagnose problems in neural networks\n",
    "- Common sources of errors in ML workflows\n",
    "- Practical tools and techniques for debugging PyTorch models\n",
    "- How to interpret model outputs and training signals to identify issues\n",
    "\n",
    "By the end, you'll be able to approach ML model debugging with confidence and efficiency.\n",
    "\n",
    "## 2. Setup & Imports\n",
    "In this section, we'll set up the environment and import the necessary libraries for debugging PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30a0d972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6d20b68bd0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup: Import libraries and prepare data/model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time as time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1932e4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9).\n",
    "- train_dataset: Contains 60,000 training images and their labels.\n",
    "- test_dataset: Contains 10,000 test images and their labels.\n",
    "Each image is transformed to a tensor and normalized for better training stability.\n",
    "Data loaders (train_loader, test_loader) provide batches of data for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6372022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "def open_dataset(dataset, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    index = np.random.permutation(dataset.targets)\n",
    "    dataset.targets = torch.tensor(index)\n",
    "open_dataset(test_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51deca01",
   "metadata": {},
   "source": [
    "The model defined here is a simple Convolutional Neural Network (CNN) designed for classifying MNIST handwritten digit images. It consists of two convolutional layers followed by two fully connected layers. The convolutional layers extract spatial features from the input images, while the fully connected layers perform classification based on these features. The final output layer produces scores for each of the 10 digit classes (0-9). This architecture is commonly used for image classification tasks and serves as a solid baseline for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cfda6cd1-5922-4a99-974b-0b91504afa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95288f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple CNN model for MNIST\n",
    "class TinyMNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 2, 3, padding=1),     # (2, 28, 28)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                   # (2, 14, 14)\n",
    "\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * 14 * 14, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8480ec",
   "metadata": {},
   "source": [
    "## 3. Visualizing Data & Model\n",
    "\n",
    "Understanding your data and model architecture is a crucial first step in debugging machine learning workflows. In this section, you'll learn how to:\n",
    "\n",
    "- Visualize sample images and their corresponding labels from the MNIST training set\n",
    "- Inspect the structure and layers of the convolutional neural network (CNN) model\n",
    "- Examine model parameters to ensure correct initialization\n",
    "\n",
    "These visualizations help verify that data is loaded correctly and the model is structured as intended before proceeding to training and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e43fdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize sample inputs and labels from the training set\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "    plt.title(f\"Label: {example_targets[i].item()}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "112bf9c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get label counts\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_counts = np.bincount(\u001b[43mtrain_labels\u001b[49m, minlength=NUM_CLASSES)\n\u001b[32m      3\u001b[39m test_counts = np.bincount(test_labels, minlength=NUM_CLASSES)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Convert counts to percentages\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Get label counts\n",
    "train_counts = np.bincount(train_labels, minlength=NUM_CLASSES)\n",
    "test_counts = np.bincount(test_labels, minlength=NUM_CLASSES)\n",
    "\n",
    "# Convert counts to percentages\n",
    "train_percent = 100 * train_counts / len(train_labels)\n",
    "test_percent = 100 * test_counts / len(test_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(np.arange(NUM_CLASSES)-0.2, train_percent, width=0.4, color='skyblue', alpha=0.7, label='Train')\n",
    "plt.bar(np.arange(NUM_CLASSES)+0.2, test_percent, width=0.4, color='salmon', alpha=0.7, label='Test')\n",
    "plt.axhline(100 / NUM_CLASSES, color='grey', linestyle='--', alpha=0.5, label='Uniform (10%)')\n",
    "plt.title(\"Label Distribution (%): Train vs Test\")\n",
    "plt.xlabel(\"Digit Label\")\n",
    "plt.ylabel(\"Percentage of Dataset (%)\")\n",
    "plt.xticks(range(NUM_CLASSES))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f138274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "\n",
      "TinyMNISTNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=392, out_features=8, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=8, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model parameters:\n",
      "features.0.weight: torch.Size([2, 1, 3, 3])\n",
      "features.0.bias: torch.Size([2])\n",
      "classifier.1.weight: torch.Size([8, 392])\n",
      "classifier.1.bias: torch.Size([8])\n",
      "classifier.3.weight: torch.Size([10, 8])\n",
      "classifier.3.bias: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Visualize model architecture\n",
    "print(\"Model architecture:\\n\")\n",
    "print(model)\n",
    "\n",
    "# Visualize model parameters (layer names and shapes)\n",
    "print(\"\\nModel parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff11b43",
   "metadata": {},
   "source": [
    "## 4. Forward/Backward Pass Debugging (15 min)\n",
    "Understanding how data flows through your model and how gradients are computed is essential for effective debugging.\n",
    "\n",
    "In this section, you'll learn to:\n",
    "- Inspect activations and outputs at each layer\n",
    "- Check gradients to ensure proper learning\n",
    "- Use hooks and manual inspection to debug the forward and backward passes\n",
    "- Identify issues such as vanishing/exploding gradients or incorrect output shapes\n",
    "\n",
    "We'll walk through practical examples using PyTorch's autograd and hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a85797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, num_epochs: int):\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"train_loss_steps\": []  # per-batch loss trace\n",
    "    }\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # --- Training phase ---\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        step_losses = []\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data, target\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # record\n",
    "            step_loss = loss.item()\n",
    "            step_losses.append(step_loss)\n",
    "            running_loss += step_loss\n",
    "\n",
    "            preds = output.argmax(dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "            # store every batch loss for visualization\n",
    "            history[\"train_loss_steps\"].append((global_step, step_loss))\n",
    "            global_step += 1\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # --- Validation phase ---\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data, target\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "                preds = output.argmax(dim=1)\n",
    "                val_correct += (preds == target).sum().item()\n",
    "                val_total += target.size(0)\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        # --- Logging ---\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d}: \"\n",
    "            f\"Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, \"\n",
    "            f\"Train Acc = {train_acc:.2f}%, Val Acc = {val_acc:.2f}%, \"\n",
    "            f\"Time = {elapsed:.1f}s, \"\n",
    "        )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc6292",
   "metadata": {},
   "source": [
    "To train a neural network in PyTorch, you need to define both an optimizer and a loss function. The optimizer updates the model parameters based on the computed gradients, while the loss function measures how well the model's predictions match the true labels.\n",
    "\n",
    "- **Optimizer:** [torch.optim documentation](https://pytorch.org/docs/stable/optim.html)\n",
    "- **Loss Function:** [torch.nn documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "Particularly, the _Adam_ optimizer is a popular choice for training deep learning models. It combines the advantages of two other extensions of stochastic gradient descent: _AdaGrad_ and _RMSProp_. _Adam_ adapts the learning rate for each parameter and uses estimates of first and second moments of the gradients to provide efficient and robust training. Learn more about _Adam_ in the [Adam optimizer documentation](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9274cfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss = 0.3324, Val Loss = 10.7572, Train Acc = 89.76%, Val Acc = 9.88%, Time = 12.9s, \n",
      "Epoch 02: Train Loss = 0.1959, Val Loss = 10.8417, Train Acc = 94.00%, Val Acc = 9.96%, Time = 12.2s, \n",
      "Epoch 03: Train Loss = 0.1793, Val Loss = 11.2741, Train Acc = 94.41%, Val Acc = 10.03%, Time = 11.8s, \n",
      "Epoch 04: Train Loss = 0.1649, Val Loss = 11.6130, Train Acc = 94.89%, Val Acc = 9.94%, Time = 11.8s, \n",
      "Epoch 05: Train Loss = 0.1602, Val Loss = 12.1857, Train Acc = 95.10%, Val Acc = 9.98%, Time = 11.9s, \n"
     ]
    }
   ],
   "source": [
    "# Set up optimizer and loss function\n",
    "model = TinyMNISTNet()\n",
    "reset_weights(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = train(model, optimizer, criterion, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93cc3a",
   "metadata": {},
   "source": [
    "## 5. Model Validation\n",
    "\n",
    "Validating your model is crucial to ensure it generalizes well to unseen data and does not simply memorize the training set. In this notebook, we use two main methods for model validation:\n",
    "\n",
    "1. **Plotting Training and Validation Losses:**  \n",
    "    By visualizing the loss curves for both the training and validation sets over each epoch, you can monitor the learning process and detect issues such as overfitting (where validation loss increases while training loss decreases) or underfitting (both losses remain high). Consistent and decreasing validation loss indicates good generalization.\n",
    "\n",
    "2. **Visualizing Predictions on Sample Data:**  \n",
    "    Examining a few examples from the dataset along with their predicted labels and confidence scores helps you qualitatively assess model performance. This can reveal systematic errors, misclassifications, or areas where the model is uncertain, guiding further debugging and improvement.\n",
    "\n",
    "3. **Extra Validation Metrics:**  \n",
    "    Calculating metrics such as accuracy per label, confusion matrices, or heatmaps provides deeper insight into model performance across different classes. These metrics help identify if the model is biased toward certain labels or struggles with specific digits, enabling targeted improvements.\n",
    "\n",
    "These validation techniques provide both quantitative and qualitative insights into your model's behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681bf29-eafe-4dde-8c8b-914c0c3b7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    train_loss = np.array(history[\"train_loss\"]).flatten()\n",
    "    val_loss   = np.array(history[\"val_loss\"]).flatten()\n",
    "    train_acc  = np.array(history[\"train_acc\"]).flatten()\n",
    "    val_acc    = np.array(history[\"val_acc\"]).flatten()\n",
    "    epochs = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # (1) Loss over epochs\n",
    "    axs[0].plot(epochs, train_loss, 'o-', label='Train', color='tab:blue')\n",
    "    axs[0].plot(epochs, val_loss, 'o-', label='Validation', color='tab:orange')\n",
    "    axs[0].set_title(\"Loss over Epochs\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "    # (2) Accuracy over epochs\n",
    "    axs[1].plot(epochs, train_acc, 'o-', label='Train', color='tab:green')\n",
    "    axs[1].plot(epochs, val_acc, 'o-', label='Validation', color='tab:red')\n",
    "    axs[1].set_title(\"Accuracy over Epochs\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True, linestyle='--', alpha=0.4)\n",
    "    axs[1].set_xlim(1, len(epochs))\n",
    "    #axs[1].set_ylim(0, 100)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43537c78-9868-4fd9-8285-8de8c9549ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40264e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample inputs and model predictions\n",
    "model.eval()\n",
    "\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "# If your model expects tensors on GPU:\n",
    "# example_data, example_targets = example_data.to(device), example_targets.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(example_data)\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    probs = torch.softmax(outputs, dim=1)  # convert logits to probabilities\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    img = example_data[i]\n",
    "    if img.ndim == 3 and img.shape[0] == 1:\n",
    "        plt.imshow(img[0], cmap=\"gray\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "    \n",
    "    label = example_targets[i].item()\n",
    "    pred = preds[i].item()\n",
    "    prob = 100 * probs[i, pred].item()\n",
    "\n",
    "    plt.title(f\"Label: {label}\\nPred: {pred} ({prob:.0f}%)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get all predictions and true labels from the test set\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        preds = output.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(10)\n",
    "plt.xticks(tick_marks, tick_marks)\n",
    "plt.yticks(tick_marks, tick_marks)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26afabe9",
   "metadata": {},
   "source": [
    "## 6. Common Pitfalls and How to Fix Them\n",
    "Machine learning models often fail due to subtle bugs or data issues. Recognizing common pitfalls can save significant debugging time.\n",
    "\n",
    "Key issues to watch for:\n",
    "- Data leakage between train/test sets\n",
    "- Incorrect loss function or output activation\n",
    "- Poor data normalization or preprocessing\n",
    "- Overfitting or underfitting\n",
    "- Vanishing/exploding gradients\n",
    "- Misaligned labels or targets\n",
    "\n",
    "We'll demonstrate how to detect and address these problems in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aae6ac",
   "metadata": {},
   "source": [
    "## 7. Practical Debugging Tools\n",
    "PyTorch and the Python ecosystem offer powerful tools for debugging ML models.\n",
    "\n",
    "Recommended tools and techniques:\n",
    "- `torch.autograd` for inspecting gradients and computation graphs\n",
    "- Forward/backward hooks for monitoring activations and gradients\n",
    "- TensorBoard for visualizing metrics and model graphs\n",
    "- Matplotlib for plotting loss, accuracy, and predictions (See section 5)\n",
    "- Printing shapes and values at key points in the model\n",
    "\n",
    "We'll show how to use these tools to quickly identify and resolve issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c2d66",
   "metadata": {},
   "source": [
    "## 8. Guided Exercise: Fix a Buggy Model\n",
    "Now it's your turn! Below is a model with intentional bugs. Try to identify and fix the issues using the debugging techniques we've covered.\n",
    "\n",
    "Steps:\n",
    "1. Run the code and observe any errors or unexpected outputs.\n",
    "2. Use visualization, hooks, and print statements to diagnose the problem.\n",
    "3. Fix the bugs and verify the model trains correctly.\n",
    "\n",
    "Discuss your findings and solutions with your peers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad36d4-8d62-44ae-a19b-32cf4c44a00c",
   "metadata": {},
   "source": [
    "### 8.1 Model does not converge\n",
    "\n",
    "The original model fails to converge to a satisfactory solution, achieving only around 10% accuracy on both the training and validation datasets. This poor performance could stem from two main causes:\n",
    "\n",
    "1. The model architecture may be inappropriate or lack sufficient capacity.\n",
    "\n",
    "2. The training configuration may require better hyperparameter tuning (e.g. learning rate, batch size, optimizer, or number of epochs).\n",
    "\n",
    "In this section, we will focus exclusively on hyperparameter optimization, investigating how different training settings affect convergence and aiming to achieve a train accuracy above 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397630af-93ed-4b96-8f92-0c5aa298d8cf",
   "metadata": {},
   "source": [
    "### 8.2 Very low validation error\n",
    "\n",
    "Even though the model achieves over 90% training accuracy with proper hyperparameters, the validation accuracy stays near 10%. This suggests that something fundamental may be wrong with how the model is trained or evaluated. Your task is to investigate and explain the cause of this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3049bc-d37e-44a0-8e71-4936fbe82606",
   "metadata": {},
   "source": [
    "### 8.3 Make it bigger\n",
    "\n",
    "In the previous exercise, we focused on improving training through hyperparameter optimization. We also fixed the issue causing very low validation accuracy. Now, we will explore the other major factor that affects model performance — the architecture. Deeper and more expressive models can capture more complex features from the data, often achieving significantly higher accuracy. For example, well-designed convolutional neural networks (CNNs) can reach up to 99% accuracy on MNIST-like datasets.\n",
    "\n",
    "Below, we provide a larger CNN architecture that unfortunately contains a few design flaws.\n",
    "Your task is to identify and correct these issues, and then modify the model to achieve at least 97% validation accuracy.\n",
    "\n",
    "### What to do\n",
    "\n",
    "The original intention behind this architecture was to build upon the TinyMNISTNet model, increasing the initial number of output channels from 2 to 8, and then adding at least one additional convolutional layer to make the network deeper. The goal is to ensure that the spatial resolution is progressively reduced through the network:\n",
    "\n",
    "From 28×28 → 14×14 after the first pooling layer\n",
    "\n",
    "From 14×14 → 7×7 after the second pooling layer\n",
    "\n",
    "This kind of progressive spatial compression is very common in deep learning architectures. It serves two main purposes:\n",
    "\n",
    "1. It enables the model to capture higher-level and more abstract features over larger receptive fields.\n",
    "\n",
    "2. It reduces computational cost by operating on smaller spatial maps in deeper layers.\n",
    "\n",
    "Importantly, as we reduce spatial dimensions, we usually increase the number of feature channels (e.g., 8 → 16 → 32). This compensates for the loss of spatial information by allowing the network to represent a richer set of learned features at each level — moving from local edge or texture detection in early layers to more complex patterns and shapes in deeper ones.\n",
    "\n",
    "Fix the model and achieve a performance over 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3c49a95-31d6-46a2-9536-19f7abfe6ba3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2268029735.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mnn.Conv2d(1, ??, 3, padding=1),\u001b[39m\n                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class BiggerMNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, ??, 3, padding=1),     \n",
    "            nn.ReLU(inplace=True),\n",
    "            ???\n",
    "            nn.Conv2d(??, ??, 3, padding=1),   \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                  \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(?? * ?? * ??, ??),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(??, ??)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01e763-4fb9-4417-bcc2-e43302322eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_big = BiggerMNISTNet()\n",
    "reset_weights(model_big)\n",
    "optimizer = optim.Adam(model_big.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = train(model_big, optimizer, criterion, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841fe8d8-f592-4e55-a6e6-e508442ba901",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4640269",
   "metadata": {},
   "source": [
    "## 9. Wrap-up & Q&A\n",
    "Congratulations on completing the debugging session!\n",
    "- Review the key techniques and tools for debugging ML models\n",
    "- Share your experiences and ask questions\n",
    "- Explore further resources for advanced debugging and model analysis\n",
    "\n",
    "Thank you for participating!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f03c69",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
