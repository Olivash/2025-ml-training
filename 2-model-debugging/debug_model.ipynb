{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b28f9a2",
   "metadata": {},
   "source": [
    "# Debugging Machine Learning Models in PyTorch\n",
    "Welcome to the hands-on session for ML model debugging! In this notebook, you'll learn practical techniques to diagnose and fix issues in neural networks using PyTorch.\n",
    "\n",
    "**Session Outline (75 min):**\n",
    "1. Introduction to debugging ML models (5 min)\n",
    "2. Setup & imports (5 min)\n",
    "3. Visualizing data and model (10 min)\n",
    "4. Forward/backward pass debugging (15 min)\n",
    "5. Common pitfalls and how to fix them (15 min)\n",
    "6. Practical debugging tools (10 min)\n",
    "7. Guided exercise: fix a buggy model (10 min)\n",
    "8. Wrap-up & Q&A (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29028f5",
   "metadata": {},
   "source": [
    "## 1. Introduction to Debugging ML Models\n",
    "Debugging is a critical skill for any machine learning practitioner. Even well-designed models can fail due to subtle bugs, data issues, or training instabilities.\n",
    "\n",
    "In this session, you will learn:\n",
    "- How to systematically diagnose problems in neural networks\n",
    "- Common sources of errors in ML workflows\n",
    "- Practical tools and techniques for debugging PyTorch models\n",
    "- How to interpret model outputs and training signals to identify issues\n",
    "\n",
    "By the end, you'll be able to approach ML model debugging with confidence and efficiency.\n",
    "\n",
    "## 2. Setup & Imports\n",
    "In this section, we'll set up the environment and import the necessary libraries for debugging PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a0d972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14adb8b8bcd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup: Import libraries and prepare data/model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb1932e4",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3984107832.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9).\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9).\n",
    "- train_dataset: Contains 60,000 training images and their labels.\n",
    "- test_dataset: Contains 10,000 test images and their labels.\n",
    "Each image is transformed to a tensor and normalized for better training stability.\n",
    "Data loaders (train_loader, test_loader) provide batches of data for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6372022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51deca01",
   "metadata": {},
   "source": [
    "The model defined here is a simple Convolutional Neural Network (CNN) designed for classifying MNIST handwritten digit images. It consists of two convolutional layers followed by two fully connected layers. The convolutional layers extract spatial features from the input images, while the fully connected layers perform classification based on these features. The final output layer produces scores for each of the 10 digit classes (0-9). This architecture is commonly used for image classification tasks and serves as a solid baseline for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95288f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model for MNIST\n",
    "class ConvNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3) # -> (32, 26, 26)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5) # -> (32, 22, 22)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5) # -> (32, 18, 18)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3) # -> (64, 16, 16)\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=5) # -> (64, 12, 12)\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel_size=5) # -> (64, 8, 8)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = ConvNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8480ec",
   "metadata": {},
   "source": [
    "## 3. Visualizing Data & Model\n",
    "\n",
    "Understanding your data and model architecture is a crucial first step in debugging machine learning workflows. In this section, you'll learn how to:\n",
    "\n",
    "- Visualize sample images and their corresponding labels from the MNIST training set\n",
    "- Inspect the structure and layers of the convolutional neural network (CNN) model\n",
    "- Examine model parameters to ensure correct initialization\n",
    "\n",
    "These visualizations help verify that data is loaded correctly and the model is structured as intended before proceeding to training and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample inputs and labels from the training set\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "    plt.title(f\"Label: {example_targets[i].item()}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize model architecture\n",
    "print(\"Model architecture:\\n\")\n",
    "print(model)\n",
    "\n",
    "# Visualize model parameters (layer names and shapes)\n",
    "print(\"\\nModel parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff11b43",
   "metadata": {},
   "source": [
    "## 4. Forward/Backward Pass Debugging (15 min)\n",
    "Understanding how data flows through your model and how gradients are computed is essential for effective debugging.\n",
    "\n",
    "In this section, you'll learn to:\n",
    "- Inspect activations and outputs at each layer\n",
    "- Check gradients to ensure proper learning\n",
    "- Use hooks and manual inspection to debug the forward and backward passes\n",
    "- Identify issues such as vanishing/exploding gradients or incorrect output shapes\n",
    "\n",
    "We'll walk through practical examples using PyTorch's autograd and hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with loss visualization\n",
    "def train(model, optimizer, loss, num_epochs: int):\n",
    "    loss_values = {\"train\": [], \"val\": []}\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            # Convert targets to one-hot encoding for MSE\n",
    "            target_onehot = F.one_hot(target, num_classes=10).float()\n",
    "            loss = criterion(output, target_onehot)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        loss_values[\"train\"].append(running_loss / len(train_loader))\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                target_onehot = F.one_hot(target, num_classes=10).float()\n",
    "                loss = criterion(output, target_onehot)\n",
    "                val_loss += loss.item()\n",
    "        loss_values[\"val\"].append(val_loss / len(test_loader))\n",
    "        elapsed_time = time.time() - start_time\n",
    "    \n",
    "        print(f\"Epoch {epoch}: Average training loss = {loss_values[\"train\"][-1]:.4f}, Validation loss = {loss_values[\"val\"][-1]:.4f}, Elapsed Time = {elapsed_time:.2f} s\")\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc6292",
   "metadata": {},
   "source": [
    "To train a neural network in PyTorch, you need to define both an optimizer and a loss function. The optimizer updates the model parameters based on the computed gradients, while the loss function measures how well the model's predictions match the true labels.\n",
    "\n",
    "- **Optimizer:** [torch.optim documentation](https://pytorch.org/docs/stable/optim.html)\n",
    "- **Loss Function:** [torch.nn documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "Particularly, the _Adam_ optimizer is a popular choice for training deep learning models. It combines the advantages of two other extensions of stochastic gradient descent: _AdaGrad_ and _RMSProp_. _Adam_ adapts the learning rate for each parameter and uses estimates of first and second moments of the gradients to provide efficient and robust training. Learn more about _Adam_ in the [Adam optimizer documentation](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9274cfdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set up optimizer and loss function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = train(model, optimizer, loss=criterion, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93cc3a",
   "metadata": {},
   "source": [
    "## 5. Model Validation\n",
    "\n",
    "Validating your model is crucial to ensure it generalizes well to unseen data and does not simply memorize the training set. In this notebook, we use two main methods for model validation:\n",
    "\n",
    "1. **Plotting Training and Validation Losses:**  \n",
    "    By visualizing the loss curves for both the training and validation sets over each epoch, you can monitor the learning process and detect issues such as overfitting (where validation loss increases while training loss decreases) or underfitting (both losses remain high). Consistent and decreasing validation loss indicates good generalization.\n",
    "\n",
    "2. **Visualizing Predictions on Sample Data:**  \n",
    "    Examining a few examples from the dataset along with their predicted labels and confidence scores helps you qualitatively assess model performance. This can reveal systematic errors, misclassifications, or areas where the model is uncertain, guiding further debugging and improvement.\n",
    "\n",
    "3. **Extra Validation Metrics:**  \n",
    "    Calculating metrics such as accuracy per label, confusion matrices, or heatmaps provides deeper insight into model performance across different classes. These metrics help identify if the model is biased toward certain labels or struggles with specific digits, enabling targeted improvements.\n",
    "\n",
    "These validation techniques provide both quantitative and qualitative insights into your model's behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5100f15e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mlosses\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(losses[\"train\"]) + 1), losses[\"train\"], label=\"train\")\n",
    "plt.plot(range(1, len(losses[\"val\"]) + 1), losses[\"val\"], label=\"validation\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40264e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample inputs and labels from the training set\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "    pred = model(example_data[0]).squeeze()\n",
    "    label = int(pred.argmax())\n",
    "    prob = 100 * pred[label]\n",
    "    plt.title(f\"Label: {example_targets[i].item()}\\n Pred: {label} ({prob:.0f} %)\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26afabe9",
   "metadata": {},
   "source": [
    "## 6. Common Pitfalls and How to Fix Them\n",
    "Machine learning models often fail due to subtle bugs or data issues. Recognizing common pitfalls can save significant debugging time.\n",
    "\n",
    "Key issues to watch for:\n",
    "- Data leakage between train/test sets\n",
    "- Incorrect loss function or output activation\n",
    "- Poor data normalization or preprocessing\n",
    "- Overfitting or underfitting\n",
    "- Vanishing/exploding gradients\n",
    "- Misaligned labels or targets\n",
    "\n",
    "We'll demonstrate how to detect and address these problems in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aae6ac",
   "metadata": {},
   "source": [
    "## 6. Practical Debugging Tools\n",
    "PyTorch and the Python ecosystem offer powerful tools for debugging ML models.\n",
    "\n",
    "Recommended tools and techniques:\n",
    "- `torch.autograd` for inspecting gradients and computation graphs\n",
    "- Forward/backward hooks for monitoring activations and gradients\n",
    "- TensorBoard for visualizing metrics and model graphs\n",
    "- Matplotlib for plotting loss, accuracy, and predictions\n",
    "- Printing shapes and values at key points in the model\n",
    "\n",
    "We'll show how to use these tools to quickly identify and resolve issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c2d66",
   "metadata": {},
   "source": [
    "## 7. Guided Exercise: Fix a Buggy Model\n",
    "Now it's your turn! Below is a model with intentional bugs. Try to identify and fix the issues using the debugging techniques we've covered.\n",
    "\n",
    "Steps:\n",
    "1. Run the code and observe any errors or unexpected outputs.\n",
    "2. Use visualization, hooks, and print statements to diagnose the problem.\n",
    "3. Fix the bugs and verify the model trains correctly.\n",
    "\n",
    "Discuss your findings and solutions with your peers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4640269",
   "metadata": {},
   "source": [
    "## 8. Wrap-up & Q&A\n",
    "Congratulations on completing the debugging session!\n",
    "- Review the key techniques and tools for debugging ML models\n",
    "- Share your experiences and ask questions\n",
    "- Explore further resources for advanced debugging and model analysis\n",
    "\n",
    "Thank you for participating!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f03c69",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aifs-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
