{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a17db8f",
   "metadata": {},
   "source": [
    "\"From Coarse to Fine: Building and Training Graph Neural Networks with Anemoi-Graphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d272f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "from IPython.display import IFrame\n",
    "\n",
    "from anemoi.graphs.edges import KNNEdges, CutOffEdges\n",
    "from anemoi.graphs.nodes import ReducedGaussianGridNodes\n",
    "from anemoi.graphs.inspect import GraphInspector\n",
    "\n",
    "from helpers import DummyDataset, train, plot_loss_curve, plot_sample\n",
    "\n",
    "# Example training loop for DownscalingModel with DataLoader\n",
    "NUM_EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 100\n",
    "HIDDEN_DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a867402",
   "metadata": {},
   "source": [
    "## 1. Create simple graph with Anemoi-Graphs\n",
    "\n",
    "We'll use `ReducedGaussianGridNodes` from `anemoi.graphs.nodes` to create two sets of nodes:\n",
    "- Coarse grid: o48 (~2 deg)\n",
    "- Fine grid: o96 (~1 deg)\n",
    "\n",
    "These represent two spatial resolutions for our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c191881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading grids from https://get.ecmwf.int/repository/anemoi/grids/grid-o48.npz\n",
      "Downloading grids from https://get.ecmwf.int/repository/anemoi/grids/grid-o96.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  input={\n",
      "    x=[10944, 2],\n",
      "    node_type='ReducedGaussianGridNodes',\n",
      "    _grid_reference_distance=0.03782872513455103,\n",
      "  },\n",
      "  target={\n",
      "    x=[40320, 2],\n",
      "    node_type='ReducedGaussianGridNodes',\n",
      "    _grid_reference_distance=0.019504681036259703,\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create coarse and fine grid nodes\n",
    "coarse_node_builder = ReducedGaussianGridNodes('o48', name='input')\n",
    "fine_node_builder = ReducedGaussianGridNodes('o96', name='target')\n",
    "\n",
    "graph = HeteroData()\n",
    "graph = coarse_node_builder.update_graph(graph)\n",
    "graph = fine_node_builder.update_graph(graph)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0f2ba",
   "metadata": {},
   "source": [
    "Each target node in the fine grid is connected to its 4 nearest neighbours from the coarse grid using KNN-based edges. This means that for every target node, the graph includes edges from the 4 closest input nodes, allowing information to flow from coarse to fine resolution during message passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28324e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  input={\n",
      "    x=[10944, 2],\n",
      "    node_type='ReducedGaussianGridNodes',\n",
      "    _grid_reference_distance=0.03782872513455103,\n",
      "  },\n",
      "  target={\n",
      "    x=[40320, 2],\n",
      "    node_type='ReducedGaussianGridNodes',\n",
      "    _grid_reference_distance=0.019504681036259703,\n",
      "  },\n",
      "  (input, to, target)={\n",
      "    edge_index=[2, 161280],\n",
      "    edge_type='KNNEdges',\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create edges from coarse to fine nodes\n",
    "edge_builder = KNNEdges(num_nearest_neighbours=4, source_name='input', target_name='target')\n",
    "# Alternatively, use CutOffEdges\n",
    "# edge_builder = CutOffEdges(cutoff_factor=0.7, source_name='input', target_name='target')\n",
    "\n",
    "graph = edge_builder.update_graph(graph)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5edcec",
   "metadata": {},
   "source": [
    "## 2. Graph Inspection\n",
    "\n",
    "This section demonstrates how to inspect and visualize the graph structure created with anemoi-graphs.  \n",
    "\n",
    "- Using `GraphInspector` from `anemoi.graphs.inspect`, you can interactively explore node types, edge connections, and graph attributes. When you run `GraphInspector(\"graphs/my_first_graph.pt\", \"interactive_plots/\").inspect()`, it creates a set of static PNG and interactive HTML files under the specified folder (`interactive_plots/`). These files allow you to visually inspect the graph structure, node distributions, and edge connections in your browser.\n",
    "- For command-line inspection, you can use the CLI tool for a summary and visualization directly in the terminal:\n",
    "\n",
    "    ```bash\n",
    "    anemoi-graphs inspect graph.pt interactive_plots/\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6658b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No isolated nodes found.\n",
      "No edge attributes found in the graph.\n",
      "No edge attributes found in the graph.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"graphs\", exist_ok=True)\n",
    "torch.save(graph, \"graphs/my_first_graph.pt\")\n",
    "GraphInspector(\"graphs/my_first_graph.pt\", \"interactive_plots/\").inspect()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac463203",
   "metadata": {},
   "source": [
    "## 3. Utility functions\n",
    "The following utility functions are provided to streamline model training and evaluation:\n",
    "\n",
    "- **train()**: Trains a GNN model using a dataset for a specified number of epochs and steps per epoch. It returns the trained model and a list of training losses.  \n",
    "    *Usage*:  \n",
    "    ```python\n",
    "            model, train_losses = train(gnn, dataset, epochs=100, steps_per_epoch=1000)\n",
    "    ```\n",
    "\n",
    "- **plot_loss_curve()**: Plots the training loss curve over epochs to visualize model convergence.  \n",
    "    *Usage*:  \n",
    "    ```python\n",
    "            plot_loss_curve(train_losses)\n",
    "    ```\n",
    "\n",
    "- **plot_sample()**: Visualizes the input (coarse), target (fine), model prediction, and error for a single sample from the dataset.  \n",
    "    *Usage*:  \n",
    "    ```python\n",
    "            plot_sample(model, dataset[0])\n",
    "    ```\n",
    "\n",
    "These functions help you quickly train, monitor, and inspect your graph neural network models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320d6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy dataset\n",
    "dataset = DummyDataset(num_samples=100, graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db2f00",
   "metadata": {},
   "source": [
    "## 4. Build your firsrt Graph Neural Network\n",
    "\n",
    "Now, let's build a simple Graph Neural Network using PyTorch Geometric (or similar). We'll use dummy node features and labels for demonstration. The core of a GNN in PyTorch Geometric is the `MessagePassing` class, which abstracts the process of exchanging information between nodes. Its workflow is structured around several key methods:\n",
    "\n",
    "- **propagate()**: The main entry point for message passing. It orchestrates the flow by calling `message()`, `aggregate()`, and `update()` in sequence for each node and its neighbors.\n",
    "- **message()**: Defines how messages are computed from source nodes `x_src` to target nodes `x_dst` along edges. You can use node features and edge attributes here.\n",
    "![MessagePassing Workflow Schema](assets/message_schema.png)\n",
    "- **aggregate()**: Specifies how incoming messages from neighbors are combined at each target node (e.g., sum, mean, max). The aggregation method is set via the `aggr` argument in the constructor.\n",
    "- **update()**: Applies a final transformation to the aggregated messages, often combining them with the target node's own features.\n",
    "![GNN Layer Structure Schema](assets/agg_update_schema.png)\n",
    "\n",
    "By customizing these methods, you can implement a wide variety of GNN architectures tailored to your graph data and task.\n",
    "\n",
    "Below are two example schema diagrams illustrating the structure of the message passing workflow in a GNN. These images help visualize how node features and edge attributes flow through the `propagate`, `message`, `aggregate`, and `update` methods.\n",
    "\n",
    "\n",
    "<!--\n",
    "class BipartiteGNN(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels_src: int,\n",
    "        in_channels_dst: int,\n",
    "        hidden_dim: int,\n",
    "        out_channels: int,\n",
    "        edge_attr_dim: int = 0,\n",
    "    ):\n",
    "        super().__init__(aggr='add')\n",
    "        self.lin_src = torch.nn.Linear(in_channels_src, hidden_dim)\n",
    "        self.lin_dst = torch.nn.Linear(in_channels_dst, hidden_dim)\n",
    "        self.lin_edges = torch.nn.Linear(edge_attr_dim, hidden_dim) if edge_attr_dim > 0 else None\n",
    "        self.projection = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x_src, x_dst, edge_index, edge_attr=None):\n",
    "        # x_src: [num_src_nodes, in_channels_src]\n",
    "        # x_dst: [num_dst_nodes, in_channels_dst]\n",
    "        # edge_index: [2, num_edges] (from src to dst)\n",
    "        out = self.propagate(\n",
    "            x=(x_src, x_dst),\n",
    "            edge_index=edge_index.to(torch.int64),\n",
    "            edge_attr=edge_attr\n",
    "        )\n",
    "        # out: [num_dst_nodes, hidden_dim]\n",
    "\n",
    "        out = self.projection(out)\n",
    "        # out: [num_dst_nodes, out_channels]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_attr=None):\n",
    "        # x_j: source node features\n",
    "        # x_j: [num_edges, in_channels_src]\n",
    "\n",
    "        m_j = self.lin_src(x_j)\n",
    "        # m_j: [num_edges, hidden_dim]\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.lin_edges(edge_attr)\n",
    "            # edge_attr: [num_edges, hidden_dim]\n",
    "\n",
    "            m_j = m_j + edge_attr\n",
    "\n",
    "        return m_j\n",
    "\n",
    "    def update(self, aggr_out, x: tuple):\n",
    "        # x: tuple (x_src, x_dst)\n",
    "\n",
    "        x_dst = self.lin_dst(x[1])\n",
    "        # x_dst: [num_dst_nodes, hidden_dim]\n",
    "\n",
    "        return aggr_out + x_dst\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d947a",
   "metadata": {},
   "source": [
    "### 4.1 Implement a dummy GNN\n",
    "\n",
    "To implement a custom GNN layer following the schemas presented below, you should focus on three core methods in the `MessagePassing` class:\n",
    "\n",
    "\n",
    "- **message(x_j, edge_attr)**: This method computes messages sent from source nodes to target nodes along each edge. According to the schema, you typically transform the source node features (and optionally edge attributes) to create the message.\n",
    "\n",
    "    *Example*:  \n",
    "    ```python\n",
    "    def message(self, x_j, edge_attr=None):\n",
    "        m_j = self.lin_src(x_j)\n",
    "        if edge_attr is not None:\n",
    "                m_j = m_j + self.lin_edges(edge_attr)\n",
    "        return m_j\n",
    "    ```\n",
    "\n",
    "![Message](assets/gnn1_message.png)\n",
    "\n",
    "- **aggregate**: This method combines all incoming messages for each target node. The aggregation operation (sum, mean, max) is set via the `aggr` argument in the constructor.  \n",
    "\n",
    "    *Example*:  \n",
    "    ```python\n",
    "    super().__init__(aggr='add')  # Use sum aggregation\n",
    "    # No need to override aggregate() unless you want a custom behavior\n",
    "    ```\n",
    "\n",
    "- **update**: After aggregation, this method applies a final transformation to the aggregated messages, often combining them with the target node's own features.  \n",
    "\n",
    "    *Example*:  \n",
    "    ```python\n",
    "    def update(self, aggr_out, x: tuple):\n",
    "        x_dst = self.lin_dst(x[1])\n",
    "        return aggr_out + x_dst\n",
    "    ```\n",
    "\n",
    "![Aggration & Update](assets/gnn1_agg_update.png)\n",
    "\n",
    "**Workflow Summary:**  \n",
    "1. **Message**: Transform source node features (and edge attributes) to create messages for each edge.  \n",
    "2. **Aggregate**: Collect and sum all messages arriving at each target node.  \n",
    "3. **Update**: Combine the aggregated message with the target node's features to produce the final output.\n",
    "\n",
    "By customizing these methods, you can implement a wide variety of GNN layers tailored to your graph structure and learning task. The provided code in the next cell demonstrates this workflow in practice.\n",
    "\n",
    "### EXERCISE:\n",
    "Fill in the `???` in the `BipartiteGNN` class below so that the linear layers have the correct input and output dimensions.  \n",
    "Hint: Use the arguments `in_channels_src`, `in_channels_dst`, `hidden_dim`, and `out_channels` to ensure the shapes match the message passing workflow described above.\n",
    "\n",
    "- `self.lin_src = torch.nn.Linear(in_channels_src, ???)`\n",
    "- `self.lin_dst = torch.nn.Linear(???, hidden_dim)`\n",
    "- `self.projection = nn.Linear(???, ???)`\n",
    "\n",
    "Try to reason about the shapes at each step and refer to the schema diagrams for guidance.  \n",
    "Once you have filled in the blanks, run the cell to check if your implementation works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "457768ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4010640730.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mself.lin_src = torch.nn.Linear(in_channels_src, ???)\u001b[39m\n                                                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class BipartiteGNN(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels_src: int,\n",
    "        in_channels_dst: int,\n",
    "        hidden_dim: int,\n",
    "        out_channels: int,\n",
    "        edge_attr_dim: int = 0,\n",
    "    ):\n",
    "        super().__init__(aggr='add')\n",
    "        self.lin_src = torch.nn.Linear(in_channels_src, ???)\n",
    "        self.lin_dst = torch.nn.Linear(???, hidden_dim)\n",
    "        self.lin_edges = torch.nn.Linear(edge_attr_dim, hidden_dim) if edge_attr_dim > 0 else None\n",
    "        self.projection = nn.Linear(???, ???)\n",
    "\n",
    "    def forward(self, x_src, x_dst, edge_index, edge_attr=None):\n",
    "        # x_src: [num_src_nodes, in_channels_src]\n",
    "        # x_dst: [num_dst_nodes, in_channels_dst]\n",
    "        # edge_index: [2, num_edges] (from src to dst)\n",
    "        out = self.propagate(\n",
    "            x=(x_src, x_dst),\n",
    "            edge_index=edge_index.to(torch.int64).to(x_src.device),\n",
    "            edge_attr=edge_attr\n",
    "        )\n",
    "        # out: [num_dst_nodes, hidden_dim]\n",
    "\n",
    "        out = self.projection(out)\n",
    "        # out: [num_dst_nodes, out_channels]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_attr=None):\n",
    "        # x_j: source node features\n",
    "        # x_j: [num_edges, in_channels_src]\n",
    "\n",
    "        m_j = self.lin_src(x_j)\n",
    "        # m_j: [num_edges, hidden_dim]\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.lin_edges(edge_attr)\n",
    "            # edge_attr: [num_edges, hidden_dim]\n",
    "\n",
    "            m_j = m_j + edge_attr\n",
    "\n",
    "        return m_j\n",
    "\n",
    "    def update(self, aggr_out, x: tuple):\n",
    "        # x: tuple (x_src, x_dst)\n",
    "\n",
    "        x_dst = self.lin_dst(x[1])\n",
    "        # x_dst: [num_dst_nodes, hidden_dim]\n",
    "\n",
    "        return aggr_out + x_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243d934",
   "metadata": {},
   "source": [
    "## 5. Train the GNN on Dummy Data\n",
    "\n",
    "Let's train the model for a few epochs and observe the loss. This is a demonstration with random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edada9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = BipartiteGNN(\n",
    "    in_channels_src=dataset.num_variables, \n",
    "    in_channels_dst=2,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    out_channels=dataset.num_variables,\n",
    "    edge_attr_dim=0,\n",
    ")\n",
    "model, train_losses = train(gnn, dataset, epochs=NUM_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3440dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(model, dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832cbee7",
   "metadata": {},
   "source": [
    "## 6. Extra I\n",
    "\n",
    "- Implement attentional convolution\n",
    "- Increase the capacity of the projection layer. Add `LayerNorm` and `ReLU` to the projection layer. Include `mlp_hidden_ration` argument to increase the hidden dimension of the projection MLP.\n",
    "\n",
    "Before computing attention, the node features are first normalized using `LayerNorm` to stabilize training and improve convergence. Layer normalization helps ensure that the input distributions to the attention mechanism remain consistent, which is especially important when stacking multiple layers or working with varying graph structures. After normalization, the source and target node features are projected into separate query, key, and value spaces using linear layers. These projections form the basis for computing attention scores and aggregating information from neighboring nodes.\n",
    "![Graph Attention Procesing](assets/graphattention_process.png)\n",
    "\n",
    "The message creation in attentional convolution involves three main steps:\n",
    "\n",
    "1. **Projection**:  \n",
    "    - Source node features are projected into key and value vectors:  \n",
    "\n",
    "        $ \\quad \\quad \\mathbf{k}_j = W_k \\mathbf{x}_j $  \n",
    "        $ \\quad \\quad \\mathbf{v}_j = W_v \\mathbf{x}_j $\n",
    "    - Target node features are projected into query vectors:  \n",
    "        $ \\quad \\quad \\mathbf{q}_i = W_q \\mathbf{x}_i $\n",
    "\n",
    "2. **Attention Score Computation**:  \n",
    "    - For each edge from source node $ j $ to target node $ i $, compute the attention coefficient using the scaled dot-product:  \n",
    "        $ \\quad \\quad \\alpha_{ij} = \\frac{(\\mathbf{q}_i \\cdot \\mathbf{k}_j)}{\\sqrt{d}} $\n",
    "\n",
    "    where $ d $ is the dimensionality of the key/query vectors, `hidden_dim`.\n",
    "\n",
    "3. **Softmax Normalization and Message Aggregation**:  \n",
    "    - Normalize the attention coefficients across all neighbors of node $ i $ using softmax:  \n",
    "      $ \\quad \\quad a_{ij} = \\mathrm{softmax}_j(\\alpha_{ij}) $\n",
    "    - The message from node $ j $ to node $ i $ is the value vector weighted by the normalized attention:  \n",
    "      $ \\quad \\quad \\mathbf{m}_{ij} = a_{ij} \\mathbf{v}_j $\n",
    "    - The aggregated message at node $ i $ is the sum over all incoming messages:  \n",
    "      $ \\quad \\quad \\mathbf{m}_i = \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{m}_{ij} $\n",
    "\n",
    "This attentional mechanism allows each node to selectively focus on its most relevant neighbors during message passing.\n",
    "\n",
    "![Graph Attention Message](assets/graphattention_message.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb6ef6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28907525",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'types.UnionType' object has no attribute '__qualname__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m aggr_out + x_dst\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# DEBUG: You can use this code to debug the forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m gnn = \u001b[43mGraphTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels_src\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels_dst\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43medge_attr_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlp_hidden_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m output = gnn(\n\u001b[32m    116\u001b[39m     x_src=graph[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m].x, \n\u001b[32m    117\u001b[39m     x_dst=graph[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m].x,\n\u001b[32m    118\u001b[39m     edge_index=graph[(\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mto\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m)].edge_index.to(torch.int64),\n\u001b[32m    119\u001b[39m     edge_attr=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    120\u001b[39m )\n\u001b[32m    121\u001b[39m \u001b[38;5;28mprint\u001b[39m(output.shape)  \u001b[38;5;66;03m# Should be [num_target_nodes, out_channels]\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mGraphTransformer.__init__\u001b[39m\u001b[34m(self, in_channels_src, in_channels_dst, hidden_dim, out_channels, edge_attr_dim, mlp_hidden_ratio)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m      4\u001b[39m     in_channels_src: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     mlp_hidden_ratio: \u001b[38;5;28mint\u001b[39m = \u001b[32m4\u001b[39m,\n\u001b[32m     10\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maggr\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43madd\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mself\u001b[39m.hidden_dim = hidden_dim\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mself\u001b[39m.mlp_hidden_ratio = mlp_hidden_ratio\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ecmwf-ml/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:140\u001b[39m, in \u001b[36mMessagePassing.__init__\u001b[39m\u001b[34m(self, aggr, aggr_kwargs, flow, node_dim, decomposed_layers)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Collect attribute names requested in message passing hooks:\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28mself\u001b[39m.inspector = Inspector(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minspector\u001b[49m\u001b[43m.\u001b[49m\u001b[43minspect_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28mself\u001b[39m.inspector.inspect_signature(\u001b[38;5;28mself\u001b[39m.aggregate, exclude=[\u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33maggr\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    142\u001b[39m \u001b[38;5;28mself\u001b[39m.inspector.inspect_signature(\u001b[38;5;28mself\u001b[39m.message_and_aggregate, [\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ecmwf-ml/lib/python3.12/site-packages/torch_geometric/inspector.py:118\u001b[39m, in \u001b[36mInspector.inspect_signature\u001b[39m\u001b[34m(self, func, exclude)\u001b[39m\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# Mimic TorchScript to auto-infer `Tensor` on non-present types:\u001b[39;00m\n\u001b[32m    113\u001b[39m     param_type = Tensor \u001b[38;5;28;01mif\u001b[39;00m param_type \u001b[38;5;129;01mis\u001b[39;00m inspect._empty \u001b[38;5;28;01melse\u001b[39;00m param_type\n\u001b[32m    115\u001b[39m     param_dict[param.name] = Parameter(\n\u001b[32m    116\u001b[39m         name=param.name,\n\u001b[32m    117\u001b[39m         \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mself\u001b[39m.eval_type(param_type),\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m         type_repr=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtype_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_type\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    119\u001b[39m         default=param.default,\n\u001b[32m    120\u001b[39m     )\n\u001b[32m    122\u001b[39m return_type = signature.return_annotation\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Mimic TorchScript to auto-infer `Tensor` on non-present types:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ecmwf-ml/lib/python3.12/site-packages/torch_geometric/inspector.py:67\u001b[39m, in \u001b[36mInspector.type_repr\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtype_repr\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     66\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Returns the type hint representation of an object.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtype_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_globals\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ecmwf-ml/lib/python3.12/site-packages/torch_geometric/inspector.py:477\u001b[39m, in \u001b[36mtype_repr\u001b[39m\u001b[34m(obj, _globals)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj.\u001b[34m__module__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33mbuiltins\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.\u001b[34m__qualname__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _get_name(\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__qualname__\u001b[39;49m, obj.\u001b[34m__module__\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'types.UnionType' object has no attribute '__qualname__'"
     ]
    }
   ],
   "source": [
    "class GraphTransformer(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels_src: int,\n",
    "        in_channels_dst: int,\n",
    "        hidden_dim: int,\n",
    "        out_channels: int,\n",
    "        edge_attr_dim: int = 0,\n",
    "        mlp_hidden_ratio: int = 4,\n",
    "        pre_lnorm: bool = False,\n",
    "    ):\n",
    "        super().__init__(aggr='add')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_hidden_ratio = mlp_hidden_ratio\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "        # Define layers\n",
    "        self.layer_norm_attention_src = nn.LayerNorm(normalized_shape=in_channels_src)\n",
    "        self.layer_norm_attention_dst = nn.LayerNorm(normalized_shape=in_channels_dst)\n",
    "\n",
    "        self.lin_key = nn.Linear(in_channels_src, self.hidden_dim)\n",
    "        self.lin_query = nn.Linear(in_channels_dst, self.hidden_dim)\n",
    "        self.lin_value = nn.Linear(in_channels_src, self.hidden_dim)\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, self.hidden_dim)\n",
    "        self.lin_edge = nn.Linear(edge_attr_dim, self.hidden_dim)\n",
    "\n",
    "        # self.projection = nn.Linear(self.hidden_dim, out_channels)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.mlp_hidden_ratio * self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_hidden_ratio * self.hidden_dim, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_src, x_dst, edge_index, edge_attr):        \n",
    "        query, key, value, edge_attr = self.get_qkve(x_src, x_dst, edge_attr)\n",
    "        # query: [num_dst_nodes, hidden_dim]\n",
    "        # key: [num_src_nodes, hidden_dim]\n",
    "        # value: [num_src_nodes, hidden_dim]\n",
    "\n",
    "        out = self.propagate(\n",
    "            edge_index=edge_index.to(torch.int64).to(x_src.device),\n",
    "            x=(x_src, x_dst),\n",
    "            size=(x_src.size(0), x_dst.size(0)),\n",
    "            edge_attr=edge_attr,\n",
    "            query=query,\n",
    "            key=key,\n",
    "            value=value,\n",
    "        )\n",
    "        # out: [num_dst_nodes, hidden_dim]\n",
    "\n",
    "        out = self.projection(out)\n",
    "        # out: [num_dst_nodes, out_channels]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_qkve(self, x_src, x_dst, edge_attr):\n",
    "        if self.pre_lnorm:\n",
    "            x_src = self.layer_norm_attention_src(x_src)\n",
    "            x_dst = self.layer_norm_attention_dst(x_dst)\n",
    "\n",
    "        query = self.lin_query(x_dst)\n",
    "        key = self.lin_key(x_src)\n",
    "        value = self.lin_value(x_src)\n",
    "    \n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.lin_edge(edge_attr)\n",
    "\n",
    "        return query, key, value, edge_attr\n",
    "\n",
    "    def message(\n",
    "        self,\n",
    "        query_i: torch.Tensor,\n",
    "        key_j: torch.Tensor,\n",
    "        value_j: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "        index: torch.Tensor,\n",
    "        ptr: torch.Tensor,\n",
    "        size_i: int,\n",
    "    ) -> torch.Tensor:\n",
    "        # query_i, key_j, value_j: [num_edges, hidden_dim]\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            key_j = key_j + edge_attr\n",
    "\n",
    "        # Compute attention coefficients\n",
    "        alpha = (query_i * key_j).sum(dim=-1) / self.hidden_dim ** 0.5\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        # alpha: [num_edges]\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            value_j = value_j + self.lin_edge(edge_attr)\n",
    "\n",
    "        out = value_j# * alpha.view(-1, 1)\n",
    "        # out: [num_edges, hidden_dim]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, x: tuple):\n",
    "        # x: tuple (x_src, x_dst)\n",
    "\n",
    "        x_dst = self.lin_dst(x[1])\n",
    "        # x_dst: [num_dst_nodes, num_heads x hidden_dim]\n",
    "\n",
    "        return aggr_out + x_dst\n",
    "\n",
    "\n",
    "# DEBUG: You can use this code to debug the forward pass\n",
    "gnn = GraphTransformer(\n",
    "    in_channels_src=2, \n",
    "    in_channels_dst=2,\n",
    "    hidden_dim=16,\n",
    "    out_channels=6,\n",
    "    edge_attr_dim=1,\n",
    "    mlp_hidden_ratio=4,\n",
    ")\n",
    "output = gnn(\n",
    "    x_src=graph['input'].x, \n",
    "    x_dst=graph['target'].x,\n",
    "    edge_index=graph[('input', 'to', 'target')].edge_index.to(torch.int64),\n",
    "    edge_attr=None\n",
    ")\n",
    "print(output.shape)  # Should be [num_target_nodes, out_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98febf52",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'types.UnionType' object has no attribute '__qualname__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m gnn = \u001b[43mGraphTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels_src\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels_dst\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHIDDEN_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43medge_attr_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m model, train_losses = train(gnn, dataset, epochs=NUM_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mGraphTransformer.__init__\u001b[39m\u001b[34m(self, in_channels_src, in_channels_dst, hidden_dim, out_channels, edge_attr_dim, mlp_hidden_ratio)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m      4\u001b[39m     in_channels_src: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     mlp_hidden_ratio: \u001b[38;5;28mint\u001b[39m = \u001b[32m4\u001b[39m,\n\u001b[32m     10\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maggr\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43madd\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mself\u001b[39m.hidden_dim = hidden_dim\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mself\u001b[39m.mlp_hidden_ratio = mlp_hidden_ratio\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ecmwf-ml/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:140\u001b[39m, in \u001b[36mMessagePassing.__init__\u001b[39m\u001b[34m(self, aggr, aggr_kwargs, flow, node_dim, decomposed_layers)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Collect attribute names requested in message passing hooks:\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28mself\u001b[39m.inspector = Inspector(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minspector\u001b[49m\u001b[43m.\u001b[49m\u001b[43minspect_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28mself\u001b[39m.inspector.inspect_signature(\u001b[38;5;28mself\u001b[39m.aggregate, exclude=[\u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33maggr\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    142\u001b[39m \u001b[38;5;28mself\u001b[39m.inspector.inspect_signature(\u001b[38;5;28mself\u001b[39m.message_and_aggregate, [\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ecmwf-ml/lib/python3.12/site-packages/torch_geometric/inspector.py:118\u001b[39m, in \u001b[36mInspector.inspect_signature\u001b[39m\u001b[34m(self, func, exclude)\u001b[39m\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# Mimic TorchScript to auto-infer `Tensor` on non-present types:\u001b[39;00m\n\u001b[32m    113\u001b[39m     param_type = Tensor \u001b[38;5;28;01mif\u001b[39;00m param_type \u001b[38;5;129;01mis\u001b[39;00m inspect._empty \u001b[38;5;28;01melse\u001b[39;00m param_type\n\u001b[32m    115\u001b[39m     param_dict[param.name] = Parameter(\n\u001b[32m    116\u001b[39m         name=param.name,\n\u001b[32m    117\u001b[39m         \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mself\u001b[39m.eval_type(param_type),\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m         type_repr=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtype_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_type\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    119\u001b[39m         default=param.default,\n\u001b[32m    120\u001b[39m     )\n\u001b[32m    122\u001b[39m return_type = signature.return_annotation\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Mimic TorchScript to auto-infer `Tensor` on non-present types:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ecmwf-ml/lib/python3.12/site-packages/torch_geometric/inspector.py:67\u001b[39m, in \u001b[36mInspector.type_repr\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtype_repr\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     66\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Returns the type hint representation of an object.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtype_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_globals\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ecmwf-ml/lib/python3.12/site-packages/torch_geometric/inspector.py:477\u001b[39m, in \u001b[36mtype_repr\u001b[39m\u001b[34m(obj, _globals)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj.\u001b[34m__module__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33mbuiltins\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.\u001b[34m__qualname__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _get_name(\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__qualname__\u001b[39;49m, obj.\u001b[34m__module__\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'types.UnionType' object has no attribute '__qualname__'"
     ]
    }
   ],
   "source": [
    "gnn = GraphTransformer(\n",
    "    in_channels_src=dataset.num_variables, \n",
    "    in_channels_dst=2,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    out_channels=dataset.num_variables,\n",
    "    edge_attr_dim=0,\n",
    ")\n",
    "model, train_losses = train(gnn, dataset, epochs=NUM_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7b8baaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plot_loss_curve(\u001b[43mtrain_losses\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss_curve(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ffa3660",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plot_sample(\u001b[43mmodel\u001b[49m, dataset[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "plot_sample(model, dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1930c",
   "metadata": {},
   "source": [
    "## Extra II\n",
    "\n",
    "In this final exercise, you'll extend the attentional convolution to support multi-head attention and optional query/key normalization. Multi-head attention enables the model to capture diverse patterns by attending to information from multiple representation subspaces. The `qk_norm` option applies normalization to the query and key vectors before computing attention scores, which can improve stability and expressiveness.\n",
    "\n",
    "Your task:\n",
    "- Implement a `MultiHeadGraphTransformer` layer that supports multiple attention heads.\n",
    "- Add a `qk_norm` argument to optionally normalize query and key vectors.\n",
    "- Verify your implementation by running the provided code and inspecting the output shapes and training behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphTransformer(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels_src: int,\n",
    "        in_channels_dst: int,\n",
    "        hidden_dim: int,\n",
    "        out_channels: int,\n",
    "        num_heads: int = 1,\n",
    "        edge_attr_dim: int = 0,\n",
    "        pre_lnorm: bool = False,\n",
    "        qk_norm: bool = False,\n",
    "        mlp_hidden_ratio: int = 4,\n",
    "    ):\n",
    "        super().__init__(aggr='add')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_hidden_ratio = mlp_hidden_ratio\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "        self.num_heads = num_heads\n",
    "        self.qk_norm = qk_norm\n",
    "\n",
    "        # Define layers\n",
    "        self.layer_norm_attention_src = nn.LayerNorm(normalized_shape=in_channels_src)\n",
    "        self.layer_norm_attention_dst = nn.LayerNorm(normalized_shape=in_channels_dst)\n",
    "\n",
    "        self.lin_key = nn.Linear(in_channels_src, self.num_heads * self.hidden_dim)\n",
    "        self.lin_query = nn.Linear(in_channels_dst, self.num_heads * self.hidden_dim)\n",
    "        self.lin_value = nn.Linear(in_channels_src, self.num_heads * self.hidden_dim)\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, self.num_heads * self.hidden_dim)\n",
    "        self.lin_edge = nn.Linear(edge_attr_dim, self.num_heads * self.hidden_dim)\n",
    "\n",
    "        if self.qk_norm:\n",
    "            self.q_norm = layer_kernels.QueryNorm(self.hidden_dim)\n",
    "            self.k_norm = layer_kernels.KeyNorm(self.hidden_dim)\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=self.num_heads * self.hidden_dim),\n",
    "            nn.Linear(self.num_heads * self.hidden_dim, self.mlp_hidden_ratio * self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_hidden_ratio * self.hidden_dim, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_src, x_dst, edge_index, edge_attr):        \n",
    "        query, key, value, edge_attr = self.get_qkve(x_src, x_dst, edge_attr)\n",
    "        # query: [num_dst_nodes, num_heads x hidden_dim]\n",
    "        # key: [num_src_nodes, num_heads x hidden_dim]\n",
    "        # value: [num_src_nodes, num_heads x hidden_dim]\n",
    "    \n",
    "        out = self.propagate(\n",
    "            edge_index=edge_index.to(torch.int64).to(x_src.device),\n",
    "            x=(x_src, x_dst),\n",
    "            size=(x_src.size(0), x_dst.size(0)),\n",
    "            edge_attr=edge_attr,\n",
    "            query=query,\n",
    "            key=key,\n",
    "            value=value,\n",
    "        )\n",
    "        # out: [num_dst_nodes, num_heads x hidden_dim]\n",
    "\n",
    "        out = self.projection(out)\n",
    "        # out: [num_dst_nodes, out_channels]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_qkve(self, x_src, x_dst, edge_attr):\n",
    "        if self.pre_lnorm:\n",
    "            x_src = self.layer_norm_attention_src(x_src)\n",
    "            x_dst = self.layer_norm_attention_dst(x_dst)\n",
    "\n",
    "        query = self.lin_query(x_dst)\n",
    "        key = self.lin_key(x_src)\n",
    "        value = self.lin_value(x_src)\n",
    "\n",
    "        if self.qk_norm:\n",
    "            query = self.q_norm(query)\n",
    "            key = self.k_norm(key)\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.lin_edge(edge_attr)\n",
    "\n",
    "        return query, key, value, edge_attr\n",
    "\n",
    "    def message(\n",
    "        self,\n",
    "        query_i: torch.Tensor,\n",
    "        key_j: torch.Tensor,\n",
    "        value_j: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "        index: torch.Tensor,\n",
    "        ptr: torch.Tensor,\n",
    "        size_i: int,\n",
    "    ) -> torch.Tensor:\n",
    "        query_i = einops.rearrange(query_i, \"edges (heads vars) -> edges heads vars\", heads=self.num_heads)\n",
    "        key_j = einops.rearrange(key_j, \"edges (heads vars) -> edges heads vars\", heads=self.num_heads)\n",
    "        value_j = einops.rearrange(value_j, \"edges (heads vars) -> edges heads vars\", heads=self.num_heads)\n",
    "        # query_i, key_j, value_j: [num_edges, num_heads, hidden_dim]\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = einops.rearrange(edge_attr, \"edges (heads vars) -> edges heads vars\", heads=self.num_heads)\n",
    "            key_j = key_j + edge_attr\n",
    "\n",
    "        # Compute attention coefficients\n",
    "        alpha = (query_i * key_j).sum(dim=-1) / self.hidden_dim ** 0.5\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        # alpha: [num_edges, num_heads]\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            value_j = value_j + self.lin_edge(edge_attr)\n",
    "\n",
    "        out = value_j * alpha.view(-1, self.num_heads, 1)\n",
    "        # out: [num_edges, num_heads, hidden_dim]\n",
    "\n",
    "        out = einops.rearrange(out, \"edges heads vars -> edges (heads vars)\")\n",
    "        # out: [num_edges, num_heads x hidden_dim]\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, x: tuple):\n",
    "        # x: tuple (x_src, x_dst)\n",
    "\n",
    "        x_dst = self.lin_dst(x[1])\n",
    "        # x_dst: [num_dst_nodes, num_heads x hidden_dim]\n",
    "\n",
    "        return aggr_out + x_dst\n",
    "\n",
    "\n",
    "# DEBUG: You can use this code to debug the forward pass\n",
    "gnn = MultiHeadGraphTransformer(\n",
    "    in_channels_src=2, \n",
    "    in_channels_dst=2,\n",
    "    hidden_dim=16,\n",
    "    out_channels=6,\n",
    "    edge_attr_dim=1,\n",
    "    num_heads=3,\n",
    ")\n",
    "output = gnn(\n",
    "    x_src=graph['input'].x, \n",
    "    x_dst=graph['target'].x,\n",
    "    edge_index=graph[('input', 'to', 'target')].edge_index.to(torch.int64),\n",
    "    edge_attr=None\n",
    ")\n",
    "print(output.shape)  # Should be [num_target_nodes, out_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286991d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = MultiHeadGraphTransformer(\n",
    "    in_channels_src=dataset.num_variables, \n",
    "    in_channels_dst=2,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    out_channels=dataset.num_variables,\n",
    "    edge_attr_dim=0,\n",
    ")\n",
    "model, train_losses = train(gnn, dataset, epochs=NUM_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(model, dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbf76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
