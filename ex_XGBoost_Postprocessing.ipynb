{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df1dc0c0",
      "metadata": {
        "id": "df1dc0c0"
      },
      "source": [
        "# Bias correction with gradient-boosted trees"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1lsui3E1GpwX",
      "metadata": {
        "id": "1lsui3E1GpwX"
      },
      "source": [
        "**Authors**: [Matthew Chantry](https://www.ecmwf.int/en/about/who-we-are/staff-profiles/matthew-chantry), [Jesper Dramsch](https://www.ecmwf.int/en/about/who-we-are/staff-profiles/jesper-dramsch) and [Fenwick Cooper](https://www.physics.ox.ac.uk/our-people/cooperf)\n",
        "\n",
        "*This notebook was last tested and operational on 02/10/2025. Please [report any issues](https://github.com/ecmwf-training/ml-examples/issues).*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mvFzXzzFHJ2z",
      "metadata": {
        "id": "mvFzXzzFHJ2z"
      },
      "source": [
        ":::{admonition} Running this notebook\n",
        ":class: tip, dropdown\n",
        "This notebook can be run/accessed on the following free online platforms. Please note they are not officially supported by or linked with ECMWF. See [Running the notebooks](sec:running_notebooks) for more details.\n",
        "\n",
        "[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ecmwf-training/ml-examples/blob/develop/ex_XGBoost_Postprocessing.ipynb)\n",
        "[![kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/ecmwf-training/ml-examples/blob/develop/ex_XGBoost_Postprocessing.ipynb)\n",
        "[![binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/ecmwf-training/ml-examples/develop?labpath=ex_XGBoost_Postprocessing.ipynb)\n",
        "[![github](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/ecmwf-training/ml-examples/blob/develop/ex_XGBoost_Postprocessing.ipynb)\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lfopIR5xHo0W",
      "metadata": {
        "id": "lfopIR5xHo0W"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b5bf4rENYAK",
      "metadata": {
        "id": "8b5bf4rENYAK"
      },
      "source": [
        "In this example we will apply a gradient-boosted tree model (XGBoost) to the ECMWF forecast of surface temperature. Specifically, we are trying to predict the difference between station observations of 2m-temperature and the corresponding forecast prediction at the nearest gridpoint to the observation location. If we can accurately predict this difference (the forecast error) we can compensate for it, making the forecast more accurate. This type of approach is called *bias correction*, which is a form of statistical post-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "npT28RfVHqIf",
      "metadata": {
        "id": "npT28RfVHqIf"
      },
      "source": [
        "### About surface observation processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PhUbk4clHtKt",
      "metadata": {
        "id": "PhUbk4clHtKt"
      },
      "source": [
        "Surface observation processing refers to the process of collecting and analyzing weather observations taken at the Earth's surface by weather stations and other observing platforms. This typically includes measuring temperature, humidity, wind speed and direction, precipitation, and other meteorological variables. The observations are then used to produce weather forecasts and warnings, as well as to analyze and understand weather patterns. The process typically involves quality control checks to ensure the observations are accurate and reliable, as well as the use of automated systems to process and disseminate the observations in near real-time.\n",
        "\n",
        "Machine learning can be applied in surface observation processing in several ways. One way is to use machine learning algorithms to help improve the accuracy of forecasts by analyzing large amounts of historical weather data. The algorithms can make more accurate predictions about future weather by learning from past weather patterns. Another way machine learning can be applied is in the quality control of the observations. Machine learning models can be trained to identify and flag any observations that may be incorrect or unreliable. This quality control can help ensure that only the most accurate and reliable weather information is used for forecasts and other purposes.\n",
        "\n",
        "In addition, machine learning can be used to automate the process of surface observation processing, allowing the system to run more efficiently and quickly. This can help produce forecasts and warnings more quickly and with higher accuracy, providing more detailed and accurate information to the public.\n",
        "\n",
        "Overall, the use of Machine learning in surface observation processing can help improve the accuracy and speed of weather forecasting, and provide more detailed and accurate information to the public."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9sO_xub2-b8g",
      "metadata": {
        "id": "9sO_xub2-b8g"
      },
      "source": [
        "### Gradient boosting\n",
        "\n",
        "In this notebook we will use [XGBoost](https://xgboost.readthedocs.io/en/stable/tutorials/model.html), a popular machine learning algorithm for supervised learning (regression and classification).\n",
        "\n",
        "XGBoost implements a tree-based gradient boosting algorithm consisting of an ensemble of decision trees. Each decision tree divides the feature space into partitions and assigns a constant value, resulting in a blocky and very approximate fit - these trees are referred to as \"weak learners\". However, gradient boosting works hierarchically, such that each tree is fitted to the residuals of the ensemble of trees that came before it. By stacking many of these trees on top of each other, the result is an ensemble model that can efficiently tackle complex regression and classification problems.\n",
        "\n",
        "Among the advantages of XGBoost are that it has been proven to work very well in many applications, it handles overfitting relatively well, and has built in feature importance measures via [Shapely values](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dbf6081",
      "metadata": {
        "id": "1dbf6081"
      },
      "source": [
        "## Prepare your environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MylSvMoUIShb",
      "metadata": {
        "id": "MylSvMoUIShb"
      },
      "source": [
        "The following packages are used to process and model the data:\n",
        "\n",
        "- numpy for handling arrays and mathematical functions\n",
        "- xgboost for building boosted tree regression models\n",
        "- scikit-learn for machine learning tools ([imported as sklearn](https://pythonorp.com/scikit-learn-vs-sklearn-heres-the-actual-difference/))\n",
        "- matplotlib for plots\n",
        "- shap for estimating feature importance (see end of notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753337b2-1067-4fda-a082-d1db5b8ab5ae",
      "metadata": {
        "id": "753337b2-1067-4fda-a082-d1db5b8ab5ae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "#trying\n",
        "import matplotlib.pyplot as plt\n",
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hnV6YFvMIeto",
      "metadata": {
        "id": "hnV6YFvMIeto"
      },
      "source": [
        "Before going further, we also have to download a utility function for plotting. This is contained within a [plugin](https://github.com/mchantry/climetlab-mltc-surface-observation-postprocessing) to the [Climetlab](https://climetlab.readthedocs.io/en/latest/) package, which is no longer updated. In order to avoid a dependency on this package (which can cause errors in some environments), we load the function directly from the GitHub source code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "As1hGnJKIfbJ",
      "metadata": {
        "id": "As1hGnJKIfbJ"
      },
      "outputs": [],
      "source": [
        "# Download the raw Python file from GitHub\n",
        "!wget -O utils.py https://raw.githubusercontent.com/mchantry/climetlab-mltc-surface-observation-postprocessing/master/climetlab_mltc_surface_observation_postprocessing/utils.py\n",
        "\n",
        "# Run it in the notebook so functions are available\n",
        "%run utils.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gk9NY6AFIrHc",
      "metadata": {
        "id": "gk9NY6AFIrHc"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yCw6F1oJIrr8",
      "metadata": {
        "id": "yCw6F1oJIrr8"
      },
      "source": [
        "We will now download the dataset: this comprises 36-hour forecast errors of 2m-temperature from a ECMWF's high resolution forecast system, using station observations as the truth. Currently there are three variables in the dataset:\n",
        "\n",
        "- `forecast_error`: the difference between the forecasted value of 2m-temperature and the observed value, in °C.\n",
        "- `time_of_day`: the local time of day, in decimal hours. Useful for diagnosing the diurnal cycle model bias.\n",
        "- `soil_temperature`: the model soil temperature, in °C.\n",
        "\n",
        "For each variable, the dataset contains over 5 million datapoints covering around 8000 weather stations around the world (not specified in the dataset).\n",
        "\n",
        "We will load each variable separately. Notice that we reshape the explanator variables (features) to 2D arrays (albeit with a single column) for compatability with scikit-learn. This may take a minute or so depending on the speed of your connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0DTG7XD0Iza5",
      "metadata": {
        "id": "0DTG7XD0Iza5"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://object-store.os-api.cci1.ecmwf.int/sop/\"\n",
        "\n",
        "# load in variables one at a time (.reshape() puts in correct format for later)\n",
        "forecast_error = np.genfromtxt(base_url + \"forecast_error.csv\", delimiter=\",\", skip_header=1).reshape(-1, 1)\n",
        "soil_temperature = np.genfromtxt(base_url + \"soil_temperature.csv\", delimiter=\",\", skip_header=1).reshape(-1, 1)\n",
        "time_of_day = np.genfromtxt(base_url + \"time_of_day.csv\", delimiter=\",\", skip_header=1).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RvXJrkuTI5Oh",
      "metadata": {
        "id": "RvXJrkuTI5Oh"
      },
      "source": [
        "Note that an important step of data preprocessing has already been carried out in the curation of the dataset. Spurious data has already been removed from the dataset based upon a range of categories, e.g. stations with inconsistent measurement locations, repeated values, or even physically  invalid numbers (>100°C). If you are starting on a new project this is a key step to clean the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_sVaZDa68FRe",
      "metadata": {
        "id": "_sVaZDa68FRe"
      },
      "source": [
        "Quick check on the shape of the resulting data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K4L71bUA8I0X",
      "metadata": {
        "id": "K4L71bUA8I0X"
      },
      "outputs": [],
      "source": [
        "print(forecast_error.shape)\n",
        "print(soil_temperature.shape)\n",
        "print(time_of_day.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AleIgAMWI7ca",
      "metadata": {
        "id": "AleIgAMWI7ca"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ko3dgQtpJE18",
      "metadata": {
        "id": "Ko3dgQtpJE18"
      },
      "source": [
        "Now we can prepare the data for our model. This involves randomly splitting the data into training and testing sets, such that 80% of the data is in the training set and 20% in the testing set.\n",
        "\n",
        "Ideally, we would like to know the geospatial location and time of our data points, because this would allow us to design our training/test sets to ensure independence (for more info, see our [ML MOOC](https://learning.ecmwf.int/course/index.php?categoryid=1)). However, we don't have this information, so random sampling will be used. We will also not create a validation set because we are not exploring a hyperparameter space.\n",
        "\n",
        "The splitting into training and test sets can be done one in one call to the `train_test_split()` function of scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df69b984",
      "metadata": {
        "id": "df69b984"
      },
      "outputs": [],
      "source": [
        "# Split each of the predictands and predictors in a random train/test split\n",
        "# we use 80% of the data for training & 20% for testing\n",
        "(forecast_error_train, forecast_error_test,\n",
        " time_of_day_train, time_of_day_test,\n",
        " soil_temperature_train,  soil_temperature_test) = train_test_split(forecast_error,\n",
        "                                                                    time_of_day,\n",
        "                                                                    soil_temperature,\n",
        "                                                                    test_size = 0.2,\n",
        "                                                                    random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1db2707-dde5-4142-a2a0-5f8daf617e51",
      "metadata": {
        "id": "a1db2707-dde5-4142-a2a0-5f8daf617e51"
      },
      "source": [
        "**NB**: XGBoost doesn't require that the inputs be normalized. So we can go ahead and fit the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ANpyNduhN36I",
      "metadata": {
        "id": "ANpyNduhN36I"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bDU3qxd1N4dg",
      "metadata": {
        "id": "bDU3qxd1N4dg"
      },
      "source": [
        "Next, we train our boosting model. We begin by performing a univariate regression of the forecast error only on the time of day. In the following code, we create the `reg_one_var` model object, specifying five rounds of boosting, and fit this model to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d151ce8c-abb7-4342-a0f9-8edae80ab7d5",
      "metadata": {
        "id": "d151ce8c-abb7-4342-a0f9-8edae80ab7d5"
      },
      "outputs": [],
      "source": [
        "reg_one_var = xgb.XGBRegressor(n_estimators=5, eval_metric=mean_absolute_error)\n",
        "reg_one_var = reg_one_var.fit(\n",
        "    time_of_day_train, forecast_error_train,\n",
        "    # calculate the RMSE on the eval (holdout) set\n",
        "    eval_set=[(time_of_day_test, forecast_error_test)],\n",
        ")\n",
        "reg_one_var"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-MzlTG7_QNlq",
      "metadata": {
        "id": "-MzlTG7_QNlq"
      },
      "source": [
        "We will now plot the model predictions against the true values in the test set. To do this, we generate a vector of model predictions covering all hours of the day, and overlay it on the test data set for a visual comparison. Due to the large number of data points, we call our `imgBufferFromVectors()` function which creates an image based on binning the data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3ae3bf",
      "metadata": {
        "id": "cd3ae3bf"
      },
      "outputs": [],
      "source": [
        "# Let's see how our model works across all the hours of the day\n",
        "x_rep = np.arange(0,24,0.001)[...,np.newaxis]\n",
        "y = reg_one_var.predict(x_rep)\n",
        "\n",
        "# Make an image of the time of day against the forecast error\n",
        "tod_buffer, ax_extent, count = imgBufferFromVectors(time_of_day_test, forecast_error_test,\n",
        "                                                    nx=256, ny=256, extent=[],\n",
        "                                                    calc_average=False)\n",
        "\n",
        "# Plot the number of measurements at each time of day and forecast error\n",
        "plt.imshow(np.log((count==0.0)+count),\n",
        "           cmap='Blues', origin='lower',\n",
        "           extent=ax_extent, aspect='auto')\n",
        "\n",
        "plt.xlim([0, 24])\n",
        "plt.grid()\n",
        "plt.xlabel('Time of day')\n",
        "plt.ylabel(r'2m temperature error ($^\\mathrm{o}$C)')\n",
        "cb = plt.colorbar()\n",
        "cb.set_label('Log( number of measurements )')\n",
        "\n",
        "# Line of best fit\n",
        "plt.plot(x_rep,y,'red')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eDow6mfVRVEp",
      "metadata": {
        "id": "eDow6mfVRVEp"
      },
      "source": [
        "The model (red line) is better than nothing, but does not account for the variation in 2m temperature error at single time of day (eg. at 14:00).\n",
        "\n",
        "We also calculate evaluation metrics based on mean absolute error (MAE) and root mean-squared error (RMSE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c148b3d6-8603-4be1-a270-5a6ff0939136",
      "metadata": {
        "id": "c148b3d6-8603-4be1-a270-5a6ff0939136"
      },
      "outputs": [],
      "source": [
        "# Calculate Root Mean Square error of predictions:\n",
        "\n",
        "zero_test = 0.0*forecast_error_test\n",
        "print('Mean Absolute Error Uncorrected:', metrics.mean_absolute_error(zero_test, forecast_error_test))\n",
        "print('Root Mean Squared Error Uncorrected:', np.sqrt(metrics.mean_squared_error(zero_test, forecast_error_test)))\n",
        "\n",
        "forecast_corrected = forecast_error_test.squeeze() - reg_one_var.predict(time_of_day_test)\n",
        "\n",
        "print('Mean Absolute Error Corrected:', metrics.mean_absolute_error(zero_test, forecast_corrected))\n",
        "print('Root Mean Squared Error Corrected:', np.sqrt(metrics.mean_squared_error(zero_test, forecast_corrected)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d2794b5",
      "metadata": {
        "id": "3d2794b5"
      },
      "source": [
        "## Adding more predictors\n",
        "\n",
        "Next we will add a second predictor, the model soil temperature. To understand how soil temperature is related with time of day, and the forecast error, we first create a 2D heatmap to visualise the relationship between these variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b379f24d",
      "metadata": {
        "id": "b379f24d"
      },
      "outputs": [],
      "source": [
        "# Make image of the error with the new predictor\n",
        "buffer, ax_extent, count = imgBufferFromVectors(soil_temperature_test,\n",
        "                                                time_of_day_test,\n",
        "                                                forecast_error_test,\n",
        "                                                128, 256,\n",
        "                                                extent = [],\n",
        "                                                calc_average=True)\n",
        "# Plot the image of the error\n",
        "plt.imshow(buffer, vmin=-5, vmax=5, cmap='seismic', origin='lower',\n",
        "           extent=ax_extent, aspect='auto')\n",
        "\n",
        "plt.grid()\n",
        "plt.xlabel('Soil temperature ($^o$C)')\n",
        "plt.ylabel(\"Local time of day (hours)\")\n",
        "cb = plt.colorbar()\n",
        "cb.set_label('Forecast - Observation ($^o$C)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2055228",
      "metadata": {
        "id": "e2055228"
      },
      "source": [
        "The plot shows a clear structure, such that in the evening the forecast is too warm if the soil is frozen and too cold if the soil is not. Can we learn a good representation of this error pattern?\n",
        "\n",
        "To do this, we add the second variable to our XGBoost model. This simply involves concatenating the soil temperature array onto the time of day array, for both the training and test datasets. Then, we retrain the model. We also increase the number of boosting rounds to ten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285729c8",
      "metadata": {
        "id": "285729c8"
      },
      "outputs": [],
      "source": [
        "#Create the input features\n",
        "X_train = np.concatenate([time_of_day_train, soil_temperature_train],axis=-1)\n",
        "X_test = np.concatenate([time_of_day_test, soil_temperature_test],axis=-1)\n",
        "\n",
        "reg_two_vars = xgb.XGBRegressor(n_estimators=10, eval_metric=mean_absolute_error)\n",
        "reg_two_vars = reg_two_vars.fit(\n",
        "    X_train, forecast_error_train,\n",
        "    eval_set=[(X_test, forecast_error_test)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BODdOhyxSWCP",
      "metadata": {
        "id": "BODdOhyxSWCP"
      },
      "source": [
        "Again, we'll check our MAE and RMSE evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0baf12e",
      "metadata": {
        "id": "a0baf12e"
      },
      "outputs": [],
      "source": [
        "# Calculate Root Mean Square error of predictions:\n",
        "\n",
        "zero_test = 0.0*forecast_error_test\n",
        "print('Mean Absolute Error Uncorrected:', metrics.mean_absolute_error(zero_test, forecast_error_test))\n",
        "print('Root Mean Squared Error Uncorrected:', np.sqrt(metrics.mean_squared_error(zero_test, forecast_error_test)))\n",
        "\n",
        "forecast_corrected = forecast_error_test.squeeze() - reg_two_vars.predict(X_test)\n",
        "\n",
        "print('Mean Absolute Error Corrected:', metrics.mean_absolute_error(zero_test, forecast_corrected))\n",
        "print('Root Mean Squared Error Corrected:', np.sqrt(metrics.mean_squared_error(zero_test, forecast_corrected)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af4c9c2a",
      "metadata": {
        "id": "af4c9c2a"
      },
      "source": [
        "By using two predictors we can impove the forecast of 2m-temperature by ~0.1 C. Let's visualise how the model corrects the forecast across the full 2D space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ffa23d",
      "metadata": {
        "id": "68ffa23d"
      },
      "outputs": [],
      "source": [
        "# Run the fit model over the plot domain\n",
        "\n",
        "# The x and y values of each point in the plot image, this covers\n",
        "# the range of data observed in the data\n",
        "nx = buffer.shape[0]\n",
        "ny = buffer.shape[1]\n",
        "x_st = np.linspace(ax_extent[0],ax_extent[1],nx)  # Represents soil_temperature\n",
        "y_tod = np.linspace(ax_extent[2],ax_extent[3],ny)  # Represents time_of_day\n",
        "\n",
        "# Create the input data\n",
        "input_buffer = np.stack(np.meshgrid(y_tod,x_st),axis=-1)\n",
        "X_plot = input_buffer.reshape(-1,X_train.shape[-1])\n",
        "\n",
        "#Predict and reshape prediction back to 2D plot\n",
        "raw_pred = reg_two_vars.predict(X_plot)\n",
        "model_buffer = raw_pred.reshape(input_buffer.shape[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13f0ab18",
      "metadata": {
        "id": "13f0ab18"
      },
      "outputs": [],
      "source": [
        "# Plot the image of the error\n",
        "plt.imshow(buffer, vmin=-5, vmax=5, cmap='seismic', origin='lower',\n",
        "           extent=ax_extent, aspect='auto')\n",
        "\n",
        "#plt.grid()\n",
        "plt.xlabel('Soil temperature ($^o$C)')\n",
        "plt.ylabel(\"Local time of day (hours)\")\n",
        "cb = plt.colorbar()\n",
        "cb.set_label('Forecast - Observation ($^o$C)')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Plot the model\n",
        "plt.imshow((model_buffer)*(count>0), vmin=-5, vmax=5, cmap='seismic', origin='lower',\n",
        "           extent=ax_extent, aspect='auto')\n",
        "\n",
        "#plt.grid()\n",
        "plt.xlabel('Soil temperature ($^o$C)')\n",
        "plt.ylabel(\"Local time of day (hours)\")\n",
        "cb = plt.colorbar()\n",
        "cb.set_label('XGBoost model ($^o$C)')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Plot the model over the whole domain\n",
        "plt.imshow(model_buffer, vmin=-5, vmax=5, cmap='seismic', origin='lower',\n",
        "           extent=ax_extent, aspect='auto')\n",
        "\n",
        "#plt.grid()\n",
        "plt.xlabel('Soil temperature ($^o$C)')\n",
        "plt.ylabel(\"Local time of day (hours)\")\n",
        "cb = plt.colorbar()\n",
        "cb.set_label('XGBoost model ($^o$C)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782f5fc6",
      "metadata": {
        "id": "782f5fc6"
      },
      "source": [
        "As we add more predictors and complexity to our model, it becomes better and better. But, our measurements of the forecast error are not perfect. There is some noise. We don't want our model to capture this. But an overly-complex model will. We need a simpler model, or more data. Fitting a model that is too complex for the data is called \"overfitting\". Here we have a large number of observations and still a relatively small number of free parameters, so overfitting is unlikely. We see that during training our errors on the training & testing dataset are comparable.\n",
        "\n",
        "Away from where data has been provided the model does not have constraints. We should not trust this part of the feature space.\n",
        "\n",
        "We also see different model corrections for hour 23 and hour 0, when these should be closely correlated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4e7c3e0",
      "metadata": {
        "id": "f4e7c3e0"
      },
      "source": [
        "## Extensions:\n",
        "\n",
        "1. Can you beat these predictions by changing the [XGBRegressor parameters](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn)? To avoid overfitting on the test (hold-out dataset), you could split the data into a train, validation, and test dataset. Use the train and validation datasets to explore the design space of the XGBRegressor, then run your best model against the test dataset and compare it with your best neural network.\n",
        "\n",
        "2. Is there a way of building in any prior knowledge to even this simple setup? Perhaps you can encode the fact that 0 hour follows 23? Does this help the prediction?\n",
        "\n",
        "3. What is more important for the final prediction? Is it the time-of-day or the soil temperature? You may want to look at the feature importances calculated by XGBoost.\n",
        "\n",
        "4. Can you explain the different error patterns in the two-variable error plots? The neural network errors appear to be \"smoother\" than those of the XGBoost regressor - why is that?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d833525e",
      "metadata": {
        "id": "d833525e"
      },
      "outputs": [],
      "source": [
        "explainer = shap.Explainer(reg_two_vars, feature_names=[\"time_of_day\", \"soil_temperature\"])\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# visualize the first prediction's explanation\n",
        "shap.plots.waterfall(shap_values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531f186e-afc3-4bee-842a-1bda2f4efee9",
      "metadata": {
        "id": "531f186e-afc3-4bee-842a-1bda2f4efee9"
      },
      "outputs": [],
      "source": [
        "shap.plots.beeswarm(shap_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9c832a4-752f-4188-89f1-2d7c9896e8e8",
      "metadata": {
        "id": "a9c832a4-752f-4188-89f1-2d7c9896e8e8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}